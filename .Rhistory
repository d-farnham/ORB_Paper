lon <= EWD_boxes$lon.max[EWD_boxes$box == 'west'])[1]
count_lon_west = sum(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'west'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'west'])[1]
# zg[lon,lat,plev,time]
EWD_east_box = melt(ncvar_get(zg_ncdf,
varid = "zg",
start = c(start_lon_east,start_lat_east,which(plev == 70000),1),
count = c(count_lon_east,count_lat_east,1,length(time))))
EWD_east_box = EWD_east_box %>% dplyr::group_by(Var3) %>%
dplyr::summarise(east.box = mean(value))
EWD_west_box = melt(ncvar_get(zg_ncdf,
varid = "zg",
start = c(start_lon_west,start_lat_west,which(plev == 70000),1),
count = c(count_lon_west,count_lat_west,1,length(time))))
EWD_west_box = EWD_west_box %>% dplyr::group_by(Var3) %>%
dplyr::summarise(west.box = mean(value))
tmp0 = cbind(merge(EWD_east_box, EWD_west_box),time) %>% dplyr::select(east.box, west.box, time)
if(file_num == 1){tmp = tmp0}
if(file_num > 1){tmp = rbind(tmp, tmp0)}
print(file_num)
}
tmp$model = m
if(m == 1){EWD_mod = tmp}
if(m > 1){EWD_mod = rbind(EWD_mod,tmp)}
}
# all years of the GCM data have 365 days (no leap years)
# below we create the month and day of each of these no-leap year days
doy_no_leap = data.frame(month = month(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")),
day = day(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")))
# create a vector of dates for the GCM historic run
date = c(as.Date(paste0(rep(1950:2005, each = 365),"-",doy_no_leap$month,"-",doy_no_leap$day)))
# replace the date, lon, and lat indices with actual values
EWD_mod$date = rep(date,length(mmod))
EWD_mod = EWD_mod %>% dplyr::mutate(EWD = east.box - west.box)
EWD_mod = EWD_mod %>% dplyr::mutate(season = GetSeasonDate(date)) %>%
dplyr::select(date, model, EWD, season)
save(EWD_mod, file = 'Processed_Data/EWD_mod.RData')
# calculate the EWD index from the reananlysis
rm(list = ls())
source("R/GetSeasonDate.R")
load(file = 'Processed_Data/EWD_boxes.RData')
load(file = 'Processed_Data/data.file.path')
obs_output = nc_open(paste0(data.file.path,'REANALYSIS_data/NCAR_zg_700_REANALYSIS.nc'))
time = ncvar_get(obs_output, varid = "T", start = c(1), count = c(-1))
# NOTE: there are no leaps years in this data #
lat = ncvar_get(obs_output, varid = "Y", start = c(1), count = c(-1))
lon = ncvar_get(obs_output, varid = "X", start = c(1), count = c(-1))
# formatting of Z is float zg[lon,lat,plev,time]
start_lat_east = which(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'east'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'east'])[1]
count_lat_east = sum(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'east'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'east'])[1]
start_lat_west = which(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'west'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'west'])[1]
count_lat_west = sum(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'west'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'west'])[1]
start_lon_east = which(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'east'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'east'])[1]
count_lon_east = sum(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'east'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'east'])[1]
start_lon_west = which(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'west'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'west'])[1]
count_lon_west = sum(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'west'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'west'])[1]
# both boxes are 5 by 5 cells
EWD_east_box = melt(ncvar_get(obs_output,
varid = "phi",
start = c(start_lon_east,start_lat_east,1,1),
count = c(count_lon_east,count_lat_east,1,length(time))))
EWD_east_box = EWD_east_box %>% dplyr::group_by(Var3) %>%
dplyr::summarise(east.box = mean(value))
EWD_west_box = melt(ncvar_get(obs_output,
varid = "phi",
start = c(start_lon_west,start_lat_west,1,1),
count = c(count_lon_west,count_lat_west,1,length(time))))
EWD_west_box = EWD_west_box %>% dplyr::group_by(Var3) %>%
dplyr::summarise(west.box = mean(value))
EWD = cbind(merge(EWD_east_box, EWD_west_box, by = "Var3"),time)
EWD = EWD %>% dplyr::mutate(date = as.Date(time, origin = "1948-01-01 00:00:00"),
EWD = east.box - west.box,
season = GetSeasonDate(date)) %>%
dplyr::select(date, EWD, season)
EWD = EWD %>% dplyr::filter(date > "1949-12-31" &
date < "2006-01-01"	)
save(EWD, file = 'Processed_Data/EWD.RData')
# now compute teh MHC index
rm(list = ls())
source("R/GetSeasonDate.R")
# Define temp region for MHC
MHC_box = data.frame(lat.min = c(36),
lat.max = c(42),
lon.min = c(270),
lon.max = c(282.5))
save(MHC_box, file = 'Processed_Data/MHC_box.RData')
# load and process the ta data
load(file = 'Processed_Data/data.file.path')
list_ta_files = list.files(path = paste0(data.file.path,'GCM_data/historic/ta/'))
MHC_mod = data.frame()
for(m in 1:5){
mmod = c("r1i", "r2i", "r3i", "r4i", "r5i")
mmod_files = list_ta_files[grep(list_ta_files, pattern = mmod[m])]
for(file_num in 1:length(mmod_files)){
ta_ncdf = nc_open(paste0(data.file.path,'/GCM_data/historic/ta/',mmod_files[file_num]))
# extract time and lat/on from the file
lon = ncvar_get(ta_ncdf, varid = "lon", start = c(1), count = c(-1))
lat = ncvar_get(ta_ncdf, varid = "lat", start = c(1), count = c(-1))
plev = ncvar_get(ta_ncdf, varid = "plev", start = c(1), count = c(-1))
time = ncvar_get(ta_ncdf, varid = "time", start = c(1), count = c(-1))
lats_keep = lat[which(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)]
start_lat = which(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)[1]
count_lat = sum(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)[1]
start_lon = which(lon >= MHC_box$lon.min &
lon <= MHC_box$lon.max)[1]
count_lon = sum(lon >= MHC_box$lon.min &
lon <= MHC_box$lon.max)[1]
# ta[lon,lat,plev,time]
T_700_mod = melt(ncvar_get(ta_ncdf,
varid = "ta",
start = c(start_lon,start_lat,which(plev == 70000),1),
count = c(count_lon,count_lat,1,length(time))))
T_700_mod = T_700_mod %>% dplyr::mutate(lat = lats_keep[Var2])
T_700_mod = T_700_mod %>% dplyr::group_by(Var3) %>%
dplyr::summarise(T_700_mod = mean(value, na.rm = TRUE))
tmp0 = cbind(T_700_mod,time)
if(file_num == 1){tmp = tmp0}
if(file_num > 1){tmp = rbind(tmp, tmp0)}
print(file_num)
}
tmp$model = m
if(m == 1){MHC_mod = tmp}
if(m > 1){MHC_mod = rbind(MHC_mod,tmp)}
}
# all years of the GCM data have 365 days (no leap years)
# below we create the month and day of each of these no-leap year days
doy_no_leap = data.frame(month = month(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")),
day = day(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")))
# create a vector of dates for the GCM historic run
date = c(as.Date(paste0(rep(1950:2005, each = 365),"-",doy_no_leap$month,"-",doy_no_leap$day)))
MHC_mod$date = rep(date,length(mmod))
MHC_mod = MHC_mod %>% dplyr::mutate(season = GetSeasonDate(date)) %>%
dplyr::select(date, model, T_700_mod, season)
# get rid of very high and low values -- all "strange" values are in JJA, so this does not affect MAM
MHC_mod$T_700_mod[MHC_mod$T_700_mod > 1000 | MHC_mod$T_700_mod < 10] = NA
# define the MHC
MHC_mod = MHC_mod %>% dplyr::mutate(MHC = 6.1 * exp((17.625 * (T_700_mod - 273.15)) / ((T_700_mod - 273.15) + 243.04))) %>%
dplyr::group_by(season) %>%
dplyr::mutate(MHC_clim = mean(MHC, na.rm = TRUE),
MHC_anom = MHC - MHC_clim)
save(MHC_mod, file = 'Processed_Data/MHC_mod.RData')
# now compute the MHC for the reanalysis
rm(list = ls())
source("R/GetSeasonDate.R")
load(file = 'Processed_Data/MHC_box.RData')
# calculate MHC from the reananlysis
load(file = 'Processed_Data/data.file.path')
reanal_output = nc_open(paste0(data.file.path,'REANALYSIS_data/temp_700mb_REANALYSIS.nc'))
time = ncvar_get(reanal_output, varid = "T", start = c(1), count = c(-1))
lat = ncvar_get(reanal_output, varid = "Y", start = c(1), count = c(-1))
lon = ncvar_get(reanal_output, varid = "X", start = c(1), count = c(-1))
plev = ncvar_get(reanal_output, varid = "P", start = c(1), count = c(-1))
start_lat = which(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)[1]
count_lat = sum(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)[1]
lats_keep = lat[which(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)]
start_lon = which(lon >= MHC_box$lon.min &
lon <= MHC_box$lon.max)[1]
count_lon = sum(lon >= MHC_box$lon.min &
lon <= MHC_box$lon.max)[1]
T_700 = melt(ncvar_get(reanal_output,
varid = "temp",
start = c(start_lon,start_lat,1,1),
count = c(count_lon,count_lat,1,length(time))))
T_700 = T_700 %>% dplyr::mutate(lat = lats_keep[Var2])
T_700 = T_700 %>% dplyr::group_by(Var3) %>%
dplyr::summarise(T_700 = mean(value))
T_700 = cbind(T_700,time)
MHC = T_700 %>% dplyr::mutate(date = as.Date(time, origin = "1948-01-01 00:00:00"),
season = GetSeasonDate(date)) %>%
dplyr::select(date, T_700, season)
# define the MHC
MHC = MHC %>% dplyr::mutate(MHC = 6.1 * exp((17.625 * (T_700 - 273.15)) / ((T_700 - 273.15) + 243.04))) %>%
dplyr::group_by(season) %>%
dplyr::mutate(MHC_clim = mean(MHC, na.rm = TRUE),
MHC_anom = MHC - MHC_clim)
save(MHC, file = 'Processed_Data/MHC.RData')
# load the the RCP 8.5 data
# start with the precipitation
rm(list = ls())
source("R/GetSeasonDate.R")
# load the precipitation box
load(file = 'Processed_Data/pr_box.RData')
# next let's load and evaluate the GCM precipitation data
# 8.5
load(file = 'Processed_Data/data.file.path')
list_pr_files = list.files(path = paste0(data.file.path,'GCM_data/future GFDL CM3 RCP 8.5/pr/'))
pr_mod = data.frame()
for(m in 1){
mmod = c("r1i")
mmod_files = list_pr_files[grep(list_pr_files, pattern = mmod[m])]
for(file_num in 1:length(mmod_files)){
pr_ncdf = nc_open(paste0(data.file.path,'GCM_data/future GFDL CM3 RCP 8.5/pr/',mmod_files[file_num]))
# extract time and lat/on from the file
time = ncvar_get(pr_ncdf, varid = "time", start = c(1), count = c(-1))
lat = ncvar_get(pr_ncdf, varid = "lat", start = c(1), count = c(-1))
lon = ncvar_get(pr_ncdf, varid = "lon", start = c(1), count = c(-1))
start_lat = which(lat > pr_box$lat.min & lat < pr_box$lat.max)[1]
count_lat = sum(lat > pr_box$lat.min & lat < pr_box$lat.max)
start_lon = which(lon > pr_box$lon.min & lon < pr_box$lon.max)[1]
count_lon = sum(lon > pr_box$lon.min & lon < pr_box$lon.max)
tmp0 = melt(ncvar_get(pr_ncdf,
varid = "pr",
start = c(start_lon,start_lat,1),
count = c(count_lon,count_lat,length(time))))
if(file_num == 1){tmp = tmp0}
if(file_num > 1){tmp = rbind(tmp, tmp0)}
print(file_num)
}
tmp$model = m
if(m == 1){pr_mod = tmp}
if(m > 1){pr_mod = rbind(pr_mod,tmp)}
}
# all years of the GCM data have 365 days (no leap years)
# below we create the month and day of each of these no-leap year days
doy_no_leap = data.frame(month = month(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")),
day = day(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")))
# create a vector of dates for the GCM historic run
date = c(as.Date(paste0(rep(2006:2100, each = 365),"-",doy_no_leap$month,"-",doy_no_leap$day)))
# how many GCM cells exist in the study region
num.cells = nrow(unique(pr_mod[,c("Var1", "Var2")]))
# replace the date, lon, and lat indices with actual values
pr_mod$date = rep(rep(date,each = num.cells),length(mmod))
pr_mod$lon = lon[which(lon > pr_box$lon.min & lon < pr_box$lon.max)][pr_mod$Var1]
pr_mod$lat = lat[which(lat > pr_box$lat.min & lat < pr_box$lat.max)][pr_mod$Var2]
pr_mod_future = pr_mod %>% dplyr::select(date,lon,lat,model,value)
save(pr_mod_future, file = 'Processed_Data/pr_mod_future.RData')
# compute future RIPs -- using the historic ensemble mean of the 99th %-ile
rm(list = ls())
source("R/GetSeasonDate.R")
# get the historic 99th percentile threshold
load(file = 'Processed_Data/pr_mod.RData')
historic_thresh = pr_mod %>% dplyr::group_by(lat,lon,model) %>%
dplyr::summarise(cut = quantile(value, probs = 0.99)) %>%
dplyr::group_by(lat,lon) %>%
dplyr::summarise(xtr_thresh = mean(cut))
load(file = "Processed_Data/RIP_threshold")
load(file = 'Processed_Data/pr_mod_future.RData')
mod_RIP_future = merge(pr_mod_future, historic_thresh, by = c('lat', 'lon')) %>%
dplyr::mutate(xtr = ifelse(value > xtr_thresh, 1, 0)) %>%
dplyr::group_by(date,model) %>%
dplyr::summarise(RIP = ifelse((sum(xtr)/length(xtr)) > RIP_threshold, 1, 0)) %>%
dplyr::mutate(season = GetSeasonDate(date))
save(mod_RIP_future, file = 'Processed_Data/mod_RIP_future.RData')
# now compute the future EWD and MHC indices
rm(list = ls())
source("R/GetSeasonDate.R")
load(file = 'Processed_Data/EWD_boxes.RData')
# load and process the Z_700 data
load(file = 'Processed_Data/data.file.path')
list_zg_files = list.files(path = paste0(data.file.path,'GCM_data/future GFDL CM3 RCP 8.5/zg/'))
EWD_mod = data.frame()
for(m in 1:1){
mmod = c("r1i")
mmod_files = list_zg_files[grep(list_zg_files, pattern = mmod[m])]
for(file_num in 1:length(mmod_files)){
zg_ncdf = nc_open(paste0(data.file.path,'GCM_data/future GFDL CM3 RCP 8.5/zg/',mmod_files[file_num]))
# extract time and lat/on from the file
time = ncvar_get(zg_ncdf, varid = "time", start = c(1), count = c(-1))
lat = ncvar_get(zg_ncdf, varid = "lat", start = c(1), count = c(-1))
lon = ncvar_get(zg_ncdf, varid = "lon", start = c(1), count = c(-1))
lon[lon > 180] = lon[lon > 180] - 360
plev = ncvar_get(zg_ncdf, varid = "plev", start = c(1), count = c(-1))
start_lat_east = which(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'east'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'east'])[1]
count_lat_east = sum(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'east'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'east'])[1]
start_lat_west = which(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'west'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'west'])[1]
count_lat_west = sum(lat >= EWD_boxes$lat.min[EWD_boxes$box == 'west'] &
lat <= EWD_boxes$lat.max[EWD_boxes$box == 'west'])[1]
start_lon_east = which(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'east'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'east'])[1]
count_lon_east = sum(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'east'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'east'])[1]
start_lon_west = which(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'west'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'west'])[1]
count_lon_west = sum(lon >= EWD_boxes$lon.min[EWD_boxes$box == 'west'] &
lon <= EWD_boxes$lon.max[EWD_boxes$box == 'west'])[1]
# zg[lon,lat,plev,time]
EWD_east_box = melt(ncvar_get(zg_ncdf,
varid = "zg",
start = c(start_lon_east,start_lat_east,which(plev == 70000),1),
count = c(count_lon_east,count_lat_east,1,length(time))))
EWD_east_box = EWD_east_box %>% dplyr::group_by(Var3) %>%
dplyr::summarise(east.box = mean(value))
EWD_west_box = melt(ncvar_get(zg_ncdf,
varid = "zg",
start = c(start_lon_west,start_lat_west,which(plev == 70000),1),
count = c(count_lon_west,count_lat_west,1,length(time))))
EWD_west_box = EWD_west_box %>% dplyr::group_by(Var3) %>%
dplyr::summarise(west.box = mean(value))
tmp0 = cbind(merge(EWD_east_box, EWD_west_box),time) %>% dplyr::select(east.box, west.box, time)
if(file_num == 1){tmp = tmp0}
if(file_num > 1){tmp = rbind(tmp, tmp0)}
print(file_num)
}
tmp$model = m
if(m == 1){EWD_mod = tmp}
if(m > 1){EWD_mod = rbind(EWD_mod,tmp)}
}
# all years of the GCM data have 365 days (no leap years)
# below we create the month and day of each of these no-leap year days
doy_no_leap = data.frame(month = month(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")),
day = day(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")))
# create a vector of dates for the GCM historic run
date = c(as.Date(paste0(rep(2006:2100, each = 365),"-",doy_no_leap$month,"-",doy_no_leap$day)))
# replace the date, lon, and lat indices with actual values
EWD_mod$date = rep(date,length(mmod))
EWD_mod = EWD_mod %>% dplyr::mutate(EWD = east.box - west.box)
EWD_mod_future = EWD_mod %>% dplyr::mutate(season = GetSeasonDate(date)) %>%
dplyr::select(date, model, EWD, season)
save(EWD_mod_future, file = 'Processed_Data/EWD_mod_future.RData')
# now load the atmospheric temperatures
rm(list = ls())
source("R/GetSeasonDate.R")
# Define temp region for MHC
load(file = 'Processed_Data/MHC_box.RData')
# load and process the ta data
load(file = 'Processed_Data/data.file.path')
list_ta_files = list.files(path = paste0(data.file.path, 'GCM_data/future GFDL CM3 RCP 8.5/ta/'))
MHC_mod = data.frame()
for(m in 1:1){
mmod = c("r1i")
mmod_files = list_ta_files[grep(list_ta_files, pattern = mmod[m])]
for(file_num in 1:length(mmod_files)){
ta_ncdf = nc_open(paste0(data.file.path, 'GCM_data/future GFDL CM3 RCP 8.5/ta/',mmod_files[file_num]))
# extract time and lat/on from the file
lon = ncvar_get(ta_ncdf, varid = "lon", start = c(1), count = c(-1))
lat = ncvar_get(ta_ncdf, varid = "lat", start = c(1), count = c(-1))
plev = ncvar_get(ta_ncdf, varid = "plev", start = c(1), count = c(-1))
time = ncvar_get(ta_ncdf, varid = "time", start = c(1), count = c(-1))
lats_keep = lat[which(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)]
start_lat = which(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)[1]
count_lat = sum(lat >= MHC_box$lat.min &
lat <= MHC_box$lat.max)[1]
start_lon = which(lon >= MHC_box$lon.min &
lon <= MHC_box$lon.max)[1]
count_lon = sum(lon >= MHC_box$lon.min &
lon <= MHC_box$lon.max)[1]
# ta[lon,lat,plev,time]
T_700_mod = melt(ncvar_get(ta_ncdf,
varid = "ta",
start = c(start_lon,start_lat,which(plev == 70000),1),
count = c(count_lon,count_lat,1,length(time))))
T_700_mod = T_700_mod %>% dplyr::mutate(lat = lats_keep[Var2])
T_700_mod = T_700_mod %>% dplyr::group_by(Var3) %>%
dplyr::summarise(T_700_mod = mean(value, na.rm = TRUE))
tmp0 = cbind(T_700_mod,time)
if(file_num == 1){tmp = tmp0}
if(file_num > 1){tmp = rbind(tmp, tmp0)}
print(file_num)
}
tmp$model = m
if(m == 1){MHC_mod = tmp}
if(m > 1){MHC_mod = rbind(MHC_mod,tmp)}
}
# all years of the GCM data have 365 days (no leap years)
# below we create the month and day of each of these no-leap year days
doy_no_leap = data.frame(month = month(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")),
day = day(seq.Date(from = as.Date("1999-01-01"), to = as.Date("1999-12-31"), by = "1 day")))
# create a vector of dates for the GCM historic run
date = c(as.Date(paste0(rep(2006:2100, each = 365),"-",doy_no_leap$month,"-",doy_no_leap$day)))
MHC_mod$date = rep(date,length(mmod))
MHC_mod = MHC_mod %>% dplyr::mutate(season = GetSeasonDate(date)) %>%
dplyr::select(date, model, T_700_mod, season)
# get rid of very high and low values -- they are present during JJA, so it doesn't matter for MAM
MHC_mod$T_700_mod[MHC_mod$T_700_mod > 1000 | MHC_mod$T_700_mod < 10] = NA
# define the MHC
MHC_mod_future = MHC_mod %>% dplyr::mutate(MHC = 6.1 * exp((17.625 * (T_700_mod - 273.15)) / ((T_700_mod - 273.15) + 243.04)))
# need to load the historic climatology to calculate the anomaly
load(file = "Processed_Data/MHC_mod.RData")
MHC_clim = MHC_mod %>% dplyr::group_by(season) %>%
dplyr::summarise(MHC_clim = unique(MHC_clim))
MHC_mod_future = merge(MHC_mod_future, MHC_clim, by = "season") %>%
dplyr::mutate(MHC_anom = MHC - MHC_clim)
save(MHC_mod_future, file = 'Processed_Data/MHC_mod_future.RData')
# all codes run in RStudio version 1.0.136 and R version R version 3.3.3
# Older and newer versions of RStudio and R have not been tested
########################################
############### Figure 1 ###############
########################################
rm(list = ls())
package.list <- list("akima","dataRetrieval","data.table","dplyr","epitools","ggmap","ggplot2","gridExtra",
"Kendall","locfit", "lubridate","maps","ncdf4","readr","reshape2","tidyr")
source('R/load_packages.R') # clear workspace, clear console, load packages
source("R/GetSeasonDate.R")
# compare the grids from the CPC and the GCM
load(file = 'Processed_Data/CPC_grid.RData')
load(file = 'Processed_Data/mod_ORB_grid.RData')
world <- data.frame(map("world", plot=FALSE)[c("x","y")])
state <- data.frame(map("state", plot=FALSE)[c("x","y")])
source("R/load_basin_boundary.R")
pdf('Final figures/Figure_1.pdf', width = 6, height = 4.25)
ggplot() +
geom_tile(data = CPC_grid,
aes(x = (lon-360),y = lat), fill = "transparent", col = "red", size = 0.2, alpha = 0.5) +
geom_tile(data = mod_ORB_grid,
aes(x = (lon-360),y = lat), fill = "transparent", col = "blue", size = 0.6) +
geom_path(data=state, aes(x,y,z=NULL), alpha = 0.75) +
scale_y_continuous(limits = c(33,45)) +
scale_x_continuous(limits = c(-93,-75)) +
geom_polygon(aes(x = long, y = lat, group = group), alpha = 0.25, data = basin_points) +
xlab("Longitude") +
ylab("Latitude") +
theme_bw()
dev.off()
########################################
############### Figure 2 ###############
########################################
rm(list = ls())
source("R/GetSeasonDate.R")
load(file = 'Processed_Data/HCDN.RData')
load(file = 'Processed_Data/gage_info.RData')
# plot the streamflow gauges:
state <- data.frame(map("state", plot=FALSE)[c("x","y")])
source("R/load_basin_boundary.R")
load('Processed_Data/pr_box.RData')
gage_info = gage_info %>% dplyr::mutate(new_names = paste0(Short.name," (",substr(Short.name,1,1),")"))
# first part of the plot
stream_gauge_locs_plot =
ggplot() +
geom_point(data = as.data.frame(gage_info), aes(x = Longitude,y = Latitude, size=da_sq_km/10000)) +
scale_size_continuous(name = "Drainage Area (x 10000 sq. km)", limits = c(0,25), range = c(0,4)) +
geom_text(data = as.data.frame(gage_info), aes(x = Longitude+c(0,0.35,-0.35,0,0,0),
y = Latitude+c(rep(-0.5,4),0.5,-0.5),
label = new_names),
size = 3,
col = "black") +
geom_rect(data = data.frame(ymin = c(pr_box$lat.min), ymax = c(pr_box$lat.max), xmin = c(pr_box$lon.max - 360), xmax = c(pr_box$lon.min - 360)),
aes(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax), col = "red",
alpha=0.15, inherit.aes = FALSE, size = 0.5) +
geom_path(data=state, aes(x,y,z=NULL), alpha = 0.25) +
scale_y_continuous(limits = c(35.5,42.5)) +
scale_x_continuous(limits = c(-90,-77.5)) +
geom_polygon(aes(x = long, y = lat, group = group), alpha = 0.25, data = basin_points) +
ggtitle("Streamflow gauges") +
theme_bw() +
theme(legend.position = "bottom")
streamflows0 = melt(HCDN,id.vars = c("dates","streamflow")) %>%
dplyr::filter(dates > "1949-12-31" &
dates < "2006-01-01")
# calculate the 1 in 365 (or about the 99.7th percentiles) from each site
xtr_threshold = (1 - 1/365)
colnames(streamflows0) = c("date", "streamflow", "site")
streamflows = streamflows0 %>% dplyr::mutate(day = day(date),
month = month(date),
year = year(date),
streamflow = ifelse(streamflow < 0, 0 , streamflow)) %>%
dplyr::group_by(site) %>%
dplyr::mutate(xtr = quantile(streamflow, probs = xtr_threshold),
season = GetSeasonDate(date),
str_S = ifelse(streamflow >= xtr, 1, 0))
save(streamflows, file = 'Processed_Data/streamflows.RData')
flow_seasons = merge(aggregate(str_S ~ season + site, FUN = sum, data = streamflows),
aggregate(str_S ~ season + site, FUN = length, data = streamflows),
by = c("season","site"))
flow_seasons = flow_seasons %>% dplyr::mutate(prob_str_S = str_S.x/str_S.y)
flow_seasons = flow_seasons[order(flow_seasons$site),]
flow_seasons$season = factor(flow_seasons$season, levels = c("DJF", "MAM", "JJA", "SON"))
# order the sites according to drainage basin size
flow_seasons$site = factor(flow_seasons$site, levels = gage_info$Short.name[order(gage_info$D.A, decreasing = TRUE)])
prob_str_S_by_season_plot =
ggplot(data = flow_seasons) +
geom_line(aes(x = season, y = prob_str_S, group = as.factor(site), col = site), size = 1.25) +
geom_point(aes(x = season, y = prob_str_S, group = as.factor(site), col = site), size = 2.5) +
theme(legend.position = "none") +
ylab(expression(Pr(S^{s} > S[364/365]^{s}))) +
ylim(c(0,0.01)) +
theme_bw() +
theme(legend.position = "bottom")
# find the days when a RIP occured:
load('Processed_Data/CPC_mod_cell_RIP.RData')
RIP_CPC = CPC_mod_cell_RIP %>% dplyr::select(date, RIP) %>%
dplyr::mutate(RIP_sum_15 = stats::filter(RIP, rep(1, 15), method = "convolution", sides = 1))
precip_streamflow = merge(streamflows,RIP_CPC, by = "date", all.y = TRUE)
# compute the odds ratio
odds_ratio = data.frame(expand.grid(site = unique(precip_streamflow$site),
season = c("MAM"),
RIP_sum_15 = c(0)))
odds_ratio$odds_est = odds_ratio$odds_lower = odds_ratio$odds_upper = NA
for(rrow in 1:nrow(odds_ratio)){
odds_ratio_tmp = precip_streamflow %>% dplyr::mutate(RIP_over = ifelse(RIP_sum_15 > 0, 1, 0))
# only retain RIP_over == 1, and RIP_sum_15 == 0
odds_ratio_tmp = odds_ratio_tmp[odds_ratio_tmp$RIP_over == 1 |
odds_ratio_tmp$RIP_sum_15 == 0,	]
table_tmp = table(odds_ratio_tmp[odds_ratio_tmp$season == odds_ratio$season[rrow] &
odds_ratio_tmp$site == odds_ratio$site[rrow],]$RIP_over,
odds_ratio_tmp[odds_ratio_tmp$season == odds_ratio$season[rrow] &
odds_ratio_tmp$site == odds_ratio$site[rrow],]$str_S)
tryCatch({ # if there is an error, we want the loop to continue running
odds_ratio$odds_est[rrow] = oddsratio.wald(table_tmp)$measure[2,1]
odds_ratio$odds_lower[rrow] = oddsratio.wald(table_tmp)$measure[2,2]
odds_ratio$odds_upper[rrow] = oddsratio.wald(table_tmp)$measure[2,3]
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
print(rrow)
}
